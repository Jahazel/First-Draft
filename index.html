<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>AI-Powered Document Reader for Low-Vision Users</title>
    <link rel="stylesheet" href="styles.css" />
  </head>
  <body>
    <div class="container">
      <h1 id="top">AI-Powered Document Reader for Low-Vision Users</h1>
      <h2>Team Members</h2>
      <section class="team">
        <p>
          Jahazel Sanchez, Verrels Lukman Eugeneo, Asya Lyubavina, Jerry Onyango
        </p>
      </section>

      <h2>Abstract</h2>
      <p>
        In this project, we explore the effectiveness of combining traditional
        OCR with transformer-based vision-language models to create an
        accessible document reader for low-vision users. Specifically, we
        integrate Tesseract OCR with two HuggingFace models—BLIP for general
        image captioning and Donut for structured document interpretation—within
        a real-time web interface. We compare their performance on a range of
        document types and assess the system’s ability to provide contextual
        text and image interpretation through both visual and audio output. We
        find that while Tesseract provides reliable text extraction, the
        addition of vision-language models significantly enhances document
        comprehension, with Donut performing best for structured layouts and
        BLIP excelling at natural image description.
      </p>

      <h2>Introduction</h2>
      <p>
        We develop an AI-powered document reader designed to bridge this gap.
        Our system combines Tesseract OCR with vision-language models (VLMs)
        such as BLIP and Donut to produce a more comprehensive and accessible
        interpretation of documents. Rather than treating text and images as
        separate entities, our approach views documents as multimodal artifacts
        that require integrated analysis. BLIP provides descriptive captions for
        embedded images, helping users form a mental model of the visual
        content, while Donut interprets document layouts and structures without
        relying on traditional OCR pipelines. Together, these models offer
        complementary strengths that enhance both the fidelity and usability of
        document interpretation.
      </p>
      <p>
        A core objective of this project is to move beyond isolated model
        demonstrations and toward a cohesive, user-facing application. We built
        an accessible web interface using Streamlit, prioritizing screen-reader
        compatibility, intuitive navigation, and customizable outputs. Unlike
        many existing tutorials or academic examples that focus narrowly on
        performance metrics, our system emphasizes usability in real-world
        settings, particularly for users who rely on auditory or tactile
        feedback to consume content.
      </p>
      <p>
        This project differs from existing literature and open-source
        implementations by integrating multimodal models into a single,
        accessible platform designed with user experience in mind. Our work does
        not merely showcase what these models can do in theory—it demonstrates
        how they can be leveraged together to serve a specific, often
        underserved, population. In doing so, we aim to highlight the importance
        of accessibility as both a technical and ethical imperative in the
        design of intelligent systems.
      </p>

      <h2>Ethical Sweep</h2>
      <h3>General Considerations</h3>
      <p>
        At a high level, our work aims to improve accessibility for low-vision
        users by creating an AI-powered document reader that combines OCR and
        image captioning with an intuitive, inclusive interface. This tool has
        the potential to meaningfully enhance document comprehension, especially
        for materials that include complex layouts or images, which traditional
        OCR systems often struggle to interpret. However, the benefits hinge on
        careful and ethical implementation, especially regarding fairness,
        accuracy, and user privacy.
      </p>
      <p>
        While non-ML alternatives (e.g., basic OCR paired with text-to-speech)
        exist, they typically fall short in terms of interpreting image-heavy or
        structurally complex documents. Our approach, using advanced
        transformer-based models, is better suited for delivering contextual
        understanding of both visual and textual elements.
      </p>
      <p>
        Our team brings together individuals from varied technical and creative
        backgrounds, including computer science and design. However, none of us
        personally experience vision impairments. To mitigate this limitation,
        we plan to involve low-vision users throughout the development and
        testing process, ensuring that their feedback informs key design
        decisions. To handle mistakes or edge cases, we will incorporate an
        in-app feedback mechanism, allowing users to flag errors in text
        extraction or image descriptions. These reports will be reviewed
        regularly, and critical issues will be prioritized in system updates.
        Transparency will be maintained through accessible documentation
        outlining how the system functions, the models involved, and their known
        limitations.
      </p>
      <h3>Data Curation and Use</h3>
      <p>
        Our tool relies primarily on Tesseract OCR models and the SynthText
        dataset, which offer a solid starting point for general-purpose text
        recognition. These sources are well-established in the field and provide
        a strong baseline for printed text in structured layouts. However, they
        have limitations when it comes to less conventional document types—such
        as handwritten notes, tactile documents like Braille, or heavily
        cluttered layouts—which could reduce the effectiveness of our system in
        those contexts.
      </p>
      <p>
        In addition to OCR, we use image captioning models like BLIP and Donut,
        which are trained on large-scale internet datasets. While these models
        bring powerful capabilities for describing images, their training data
        may introduce several kinds of bias. One key issue is language bias,
        given the predominance of English and Latin-based scripts in both the
        OCR and captioning datasets. Another concern is layout bias, since
        synthetic datasets like SynthText may not fully capture the variability
        and disorganization of real-world documents, such as medical forms or
        personal notes. Lastly, the captioning models may reflect stereotypical
        assumptions embedded in their training data, including inaccurate
        gendering or cultural misrepresentation in image descriptions.
      </p>
      <p>
        These challenges highlight the importance of validating our tool with
        real-world user data and maintaining awareness of how dataset
        limitations might affect performance across different user groups and
        document types.
      </p>
      <h3>Impact Assessment</h3>
      <p>
        Our main ethical focus is on how the tool might affect users if it were
        actually deployed in the real world. One consideration is user reliance:
        even though we’re designing this to support low-vision users, we
        wouldn’t want the system to unintentionally replace their own ways of
        navigating content. To address this, we discussed interactive features
        like a “hints” or “progressive reveal” mode, but due to time and scope,
        we’re focusing on just making the basic tool work well first.
      </p>
      <p>
        Another potential issue is how the tool would handle documents with high
        stakes—like medical instructions, financial documents, or anything that
        requires high accuracy. We’re not aiming for full reliability in those
        contexts at this stage, but we imagine future versions could include
        accuracy warnings or prompts to verify results with a sighted assistant.
      </p>
      <p>
        Finally, we know our current system may not perform equally well on all
        document types—especially handwritten notes, complex diagrams, or
        non-English content. We’d need more testing and feedback from real users
        to understand how well the system generalizes. Long-term, it would be
        important to track those gaps and make sure improvements don’t benefit
        just one kind of user or document.
      </p>

      <h2>Related Work</h2>
      <p>
        Several previous efforts have informed and shaped our project, yet each
        contains gaps that our solution specifically targets and resolves. For
        example, Smith's ImageAssist (2021) <a href="#ref5">[5]</a> innovatively
        enabled visually impaired users to interactively explore images but
        focused primarily on isolated, exploratory interactions rather than
        integrated interpretation of mixed textual-visual content within
        documents. Our application explicitly advances beyond this by providing
        cohesive, contextually integrated descriptions for entire documents,
        thus filling a crucial accessibility gap left by ImageAssist.
      </p>
      <p>
        Similarly, Vijayanarayanan et al. (2023)
        <a href="#ref6">[6]</a> successfully combined OCR with text-to-speech
        for textual accessibility, effectively demonstrating OCR's strengths.
        However, their solution overlooks the critical task of interpreting
        visual elements and complex document layouts. By specifically
        integrating sophisticated HuggingFace transformer-based image captioning
        models, our approach directly resolves this limitation, offering
        comprehensive visual-textual integration essential for complete document
        understanding.
      </p>
      <p>
        Similarly, Vijayanarayanan et al. (2023)<a href="#ref6">[6]</a>
        demonstrated the effectiveness of combining Optical Character
        Recognition (OCR) and text-to-speech to support visually impaired users
        in efficiently processing textual content. While their approach
        highlights the strengths of OCR technology, it lacks the capability to
        interpret images or complex document layouts. Our project addresses this
        critical gap by integrating advanced HuggingFace transformer models
        specifically optimized for context-rich image captioning, ensuring
        visual elements are fully integrated into the document’s accessible
        description.
      </p>
      <p>
        Moreover, Bodi et al. (2021) <a href="#ref7">[7]</a> demonstrated
        powerful AI-driven contextual descriptions within dynamic visual
        environments (videos), developing tools such as NarrationBot and
        InfoBot. Despite their significance, these tools remain specialized for
        dynamic contexts, leaving the accessibility needs of static, image-rich
        documents largely unaddressed. Our solution specifically adapts
        AI-driven narrative generation techniques to static documents, directly
        extending AI capabilities into this underserved yet essential domain.
      </p>
      <p>
        Collectively, our project synthesizes these foundational insights while
        explicitly resolving their individual limitations, delivering a uniquely
        comprehensive, robust accessibility solution tailored precisely to the
        critical needs of low-vision users interpreting complex visual-textual
        documents.
      </p>

      <h2>Methods</h2>
      <h3>Methods Overview</h3>
      <p>
        This section outlines the methods employed to develop our AI-powered
        document reader for low-vision users. Our approach integrates optical
        character recognition (OCR) with state-of-the-art vision-language models
        (VLMs) to provide a comprehensive solution for interpreting both textual
        and visual content within documents. The core technologies powering our
        system include Tesseract OCR for extracting text and HuggingFace
        models—BLIP and Donut—for generating contextual image captions and
        interpreting document layouts. This combination enables the system to
        produce both textual and auditory outputs, ensuring a more inclusive
        experience for low-vision users. The methods section details the
        specific tools, datasets, and workflows involved, along with the steps
        taken to optimize performance and address challenges in accessibility,
        accuracy, and computational efficiency. We also highlight how user
        feedback and iterative testing will be incorporated to refine the
        system’s usability and functionality, ensuring it meets the needs of the
        target user group.
      </p>
      <h3>Software and Implementation Plan</h3>
      <p>
        To extract text from images and PDFs, Tesseract OCR will be used due to
        its open-source nature and high accuracy in recognizing characters from
        various languages and fonts. Tesseract allows seamless conversion of
        scanned documents and image-based text into readable digital content.
        This feature is particularly crucial for low-vision users who rely on
        screen readers and text-to-speech applications to access written
        information.To enhance image comprehension, pre-trained deep learning
        models from Hugging Face, such as BLIP or Donut, will be employed to
        generate descriptive captions. These models use advanced machine
        learning techniques to analyze visual content and provide meaningful
        descriptions of images. The web application will be built using an
        interactive framework to allow users to upload images and PDFs for
        processing. Two main options for development include Gradio and
        Streamlit. Gradio is particularly suited for integrating machine
        learning models with an intuitive user interface. Streamlit, on the
        other hand, provides a structured layout that is more suitable for
        applications requiring detailed document processing and structured
        navigation of extracted information.
      </p>
      <h3>OCR Using Tesseract</h3>
      <p>
        To extract readable text from scanned documents or images, we
        implemented Tesseract, an open-source OCR engine originally developed by
        Hewlett-Packard and now maintained by Google. Tesseract’s pipeline
        includes preprocessing steps such as layout analysis, line detection,
        and character segmentation, followed by a two-pass recognition system.
        In the first pass, Tesseract identifies character groupings and
        generates initial word hypotheses. These are then used to train an
        adaptive classifier that improves recognition in the second pass,
        particularly in cases where the input text is distorted, faint, or
        ambiguous. Under the hood, modern versions of Tesseract use a Long
        Short-Term Memory (LSTM)-based neural network architecture to improve
        sequence modeling and contextual accuracy. The LSTM component allows the
        model to retain relevant information across a line of text while
        filtering out noise, which is particularly beneficial when processing
        paragraphs with irregular spacing or mixed fonts. Studying R. Smith’s
        paper, “An Overview of the Tesseract OCR Engine,” helped us understand
        how Tesseract balances rule-based heuristics with neural architectures,
        especially its use of memory networks for recognizing long sequences of
        text. This fusion of traditional vision processing and recurrent neural
        networks made Tesseract an ideal candidate for our project.
      </p>
      <h3>Image Captioning with Vision-Language Transformers</h3>
      <p>
        For images embedded within documents, such as diagrams, charts, or
        photographs, we implemented transformer-based captioning models from the
        Hugging Face ecosystem. We experimented with BLIP (Bootstrapped
        Language-Image Pretraining) and Donut (Document Understanding
        Transformer), both of which generate natural language descriptions of
        visual content. These models are built using multi-modal transformers
        that combine vision encoders and language decoders, trained on
        large-scale datasets that pair images with textual descriptions. In our
        context, these models allow low-vision users to gain an understanding of
        the visual elements that are typically inaccessible to them. The use of
        transformer-based attention mechanisms—an idea central to our neural
        networks coursework—enables the models to focus on relevant visual
        features and generate coherent captions, even for complex images with
        multiple semantic layers. Unlike traditional convolutional models, these
        architectures do not rely on fixed kernel sizes or local receptive
        fields; instead, they capture global dependencies, which improves their
        performance on visually rich and information-dense documents.
      </p>
      <h3>Web Interface Implementation</h3>
      <p>
        We built our application interface using both Gradio and Streamlit, two
        Python-based web frameworks that allow for rapid deployment of machine
        learning models with a front-end layer. Gradio was used during model
        testing because of its seamless integration with Hugging Face pipelines
        and ease of debugging. Streamlit, on the other hand, was chosen for the
        final version due to its superior layout control and ability to organize
        document navigation hierarchically. The final user interface enables
        document uploads, triggers OCR and image captioning, and reads aloud the
        results via a text-to-speech module. Every design decision—from keyboard
        navigation support to semantic HTML structuring—was guided by principles
        of universal design and web accessibility.
      </p>
      <h3>Dataset Selection and Model Evaluation</h3>
      <p>
        To test and evaluate our models, we used both the Tesseract OCR training
        dataset and the SynthText dataset, which includes over 850,000 synthetic
        text images with a variety of fonts, backgrounds, and orientations. The
        dataset is particularly useful for benchmarking OCR systems because it
        includes line-level annotations and mimics real-world distortions. By
        testing on this data, we evaluated Tesseract’s robustness across diverse
        visual conditions and tuned preprocessing steps such as contrast
        enhancement and binarization. We also analyzed caption quality from BLIP
        and Donut by comparing generated outputs to ground-truth descriptions
        and evaluating them for relevance, fluency, and informativeness.
      </p>
      <h3>Postprocessing and Performance Tuning</h3>
      <p>
        Post-processing steps were critical in refining the output. OCR results
        were cleaned to remove formatting artifacts and normalize punctuation,
        while captions were filtered to discard vague or repetitive phrases. We
        also used pronunciation tuning and pacing optimization in the
        text-to-speech pipeline to ensure that the audio output was both
        intelligible and natural-sounding. To quantitatively assess performance,
        we computed precision, recall, and F1 scores on OCR word recognition and
        caption accuracy using annotated subsets of our test data. We visualized
        performance curves and confusion matrices to analyze common error
        patterns and inform further improvements.
      </p>

      <h2>Discussion and Results</h2>
      <h3>What data you will present?</h3>
      <p>
        Since our project utilizes pretrained models, we did not work with raw
        training data. Instead, we focused on testing the system using a variety
        of sample documents and images. The results we present will primarily
        include real-world examples of input documents, demonstrating how the
        tool processes and interprets both textual and visual content. This
        includes corresponding extracted text and image captions for each
        document, allowing us to assess the system’s ability to interpret both
        printed text and images with a high degree of accuracy. By presenting
        these examples, we aim to highlight the tool’s effectiveness in real-use
        scenarios, as well as its ability to handle diverse document formats.
      </p>
      <h3>How you will interpret/evaluate your data?</h3>
      <p>
        To evaluate the performance of our system, we will assess its OCR
        accuracy by calculating precision, recall, and F1 scores. These metrics
        will allow us to quantify how well the tool identifies and extracts text
        from a variety of document types, including printed, handwritten, and
        scanned materials. Additionally, we will evaluate the quality of image
        captions generated by the system using standard metrics such as BLEU and
        ROUGE scores. These metrics will provide insight into how well the
        generated captions align with human-generated descriptions of the
        images. Alongside these quantitative measures, we will incorporate human
        judgment to assess the contextual relevance and accuracy of the
        captions, ensuring that the system not only extracts text and generates
        captions but does so in a manner that reflects the content’s intended
        meaning.
      </p>
      <h3>How will you prove your point?</h3>
      <p>
        The design of our system is grounded in existing research and related
        works in the field of OCR and image captioning. Our system’s approach to
        breaking down documents—extracting text and interpreting images—mirrors
        the way a human would approach the same task. To demonstrate this, we
        will compare the system's output with our own interpretations of the
        same documents. This comparison will allow us to identify similarities
        and differences between the system’s performance and human
        interpretation, offering a baseline for evaluating the tool's
        effectiveness. By directly comparing the tool’s output to human
        interpretation, we can provide clear evidence of the system's ability to
        interpret documents accurately and meaningfully. Furthermore, this
        comparison will help us pinpoint areas where the system excels and where
        it may require further improvement, thus validating our approach and
        highlighting the strengths of the tool.
      </p>
      <h3>How your work compares to others?</h3>
      <p>
        Our approach distinguishes itself from many other projects in the field
        by focusing on the integration of pretrained models rather than
        developing and training neural networks from scratch. This decision
        allows us to concentrate on practical concerns such as system
        integration, user experience, and accessibility, rather than spending
        time on model development. While many projects in the AI and machine
        learning space prioritize the novelty of training new models or pushing
        the boundaries of technical performance, our work places a stronger
        emphasis on real-world applications. Specifically, we aim to improve the
        accessibility of document reading for low-vision users, a group often
        underserved in technology-driven solutions.
      </p>
      <p>
        By leveraging powerful existing AI tools, we were able to develop a
        system that is both effective and accessible without reinventing the
        wheel. This practical approach ensures that the system can be easily
        deployed in real-world settings, providing immediate value to users who
        rely on assistive technologies. While other projects may focus on
        technical innovation or novel model architectures, our project stands
        out by demonstrating how well-established AI tools can be applied to
        address important social needs, such as enhancing accessibility for
        individuals with visual impairments. This user-centered approach sets
        our project apart from others that may be more focused on technical
        novelty but less concerned with practical, user-oriented outcomes.
      </p>

      <h2>Reflection</h2>
      <p>
        Working on the AI-Powered Document Reader for Low-Vision Users has been
        a challenging yet rewarding experience for our team. Throughout the
        project, we deepened our understanding of neural networks, multimodal
        learning, and human-centered design, particularly as it relates to
        accessibility and inclusive technology.
      </p>
      <p>
        At the beginning, most of us were familiar with the basics of deep
        learning, but we had limited hands-on experience integrating multiple
        models or deploying real-world applications. As we navigated model
        selection — from Tesseract OCR to BLIP and Donut for image captioning —
        we learned how to evaluate not just accuracy or novelty, but also
        usability and performance in context. It became clear that no model was
        perfect across all types of documents, and this forced us to think
        critically about tradeoffs between interpretability, robustness, and
        computational efficiency.
      </p>
      <p>
        We also grew in our understanding of ethical design, particularly
        regarding how assumptions in model training, data representation, and
        interface structure can unintentionally exclude or misrepresent users
        with disabilities. This made us more deliberate in our choices and
        pushed us to think beyond technical performance toward real-world
        impact.
      </p>
      <p>
        Overall, this project allowed us to explore the intersection of AI and
        accessibility in a way that was both technically rigorous and socially
        meaningful. It gave us the confidence and curiosity to continue building
        tools that are not only intelligent but also inclusive.
      </p>

      <h2>References</h2>
      <p id="ref1">
        [1] Tesseract OCR.
        <a href="https://github.com/tesseract-ocr/tesseract"
          >https://github.com/tesseract-ocr/tesseract</a
        >
        <a href="#top">(Back to text)</a>
      </p>
      <p id="ref2">
        [2] HuggingFace Transformers.
        <a href="https://huggingface.co/transformers/"
          >https://huggingface.co/transformers/</a
        >
        <a href="#top">(Back to text)</a>
      </p>
      <p id="ref3">
        [3] Gradio.
        <a href="https://gradio.app/">https://gradio.app/</a>
        <a href="#top">(Back to text)</a>
      </p>
      <p id="ref4">
        [4] Streamlit.
        <a href="https://streamlit.io/">https://streamlit.io/</a>
        <a href="#top">(Back to text)</a>
      </p>
      <p id="ref5">
        [5] Smith, B. A. (2021). "ImageAssist: Enhancing Image Understanding for
        Low Vision Users." Proceedings of CHI.
        <a href="#top">(Back to text)</a>
      </p>
      <p id="ref6">
        [6] Vijayanarayanan et al. (2023). "Image Processing Based on Optical
        Character Recognition with Text-to-Speech for Visually Impaired."
        <em>Journal of Scientific and Engineering Research, 6(4).</em>
        <a href="#top">(Back to text)</a>
      </p>
      <p id="ref7">
        [7] Bodi et al. (2021). "Automated Video Description for Blind and Low
        Vision Users."
        <em
          >Proceedings of CHI Conference on Human Factors in Computing
          Systems.</em
        >
        <a href="#top">(Back to text)</a>
      </p>
    </div>
  </body>
</html>
