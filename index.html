<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>AI-Powered Document Reader for Low-Vision Users</title>
    <link rel="stylesheet" href="styles.css" />
  </head>
  <body>
    <div class="container">
      <h1 id="top">AI-Powered Document Reader for Low-Vision Users</h1>
      <h2>Team Members</h2>
      <section class="team">
        <p>
          Jahazel Sanchez, Verrels Lukman Eugeneo, Asya Lyubavina, Jerry Onyango
        </p>
      </section>

      <h2>Abstract</h2>
      <p>
        In this project, we explore the effectiveness of combining traditional
        OCR with transformer-based vision-language models to create an
        accessible document reader for low-vision users. Specifically, we
        integrate Tesseract OCR with two HuggingFace models—BLIP for general
        image captioning and Donut for structured document interpretation—within
        a real-time web interface. We compare their performance on a range of
        document types and assess the system’s ability to provide contextual
        text and image interpretation through both visual and audio output. We
        find that while Tesseract provides reliable text extraction, the
        addition of vision-language models significantly enhances document
        comprehension, with Donut performing best for structured layouts and
        BLIP excelling at natural image description.
      </p>

      <h2>Introduction</h2>
      <p>
        We develop an AI-powered document reader designed to bridge this gap.
        Our system combines Tesseract OCR with vision-language models (VLMs)
        such as BLIP and Donut to produce a more comprehensive and accessible
        interpretation of documents. Rather than treating text and images as
        separate entities, our approach views documents as multimodal artifacts
        that require integrated analysis. BLIP provides descriptive captions for
        embedded images, helping users form a mental model of the visual
        content, while Donut interprets document layouts and structures without
        relying on traditional OCR pipelines. Together, these models offer
        complementary strengths that enhance both the fidelity and usability of
        document interpretation.
      </p>
      <p>
        A core objective of this project is to move beyond isolated model
        demonstrations and toward a cohesive, user-facing application. We built
        an accessible web interface using Streamlit, prioritizing screen-reader
        compatibility, intuitive navigation, and customizable outputs. Unlike
        many existing tutorials or academic examples that focus narrowly on
        performance metrics, our system emphasizes usability in real-world
        settings, particularly for users who rely on auditory or tactile
        feedback to consume content.
      </p>
      <p>
        This project differs from existing literature and open-source
        implementations by integrating multimodal models into a single,
        accessible platform designed with user experience in mind. Our work does
        not merely showcase what these models can do in theory—it demonstrates
        how they can be leveraged together to serve a specific, often
        underserved, population. In doing so, we aim to highlight the importance
        of accessibility as both a technical and ethical imperative in the
        design of intelligent systems.
      </p>

      <h2>Ethical Sweep</h2>
      <h3>General Considerations</h3>
      <p>
        At a high level, our work aims to improve accessibility for low-vision
        users by creating an AI-powered document reader that combines OCR and
        image captioning with an intuitive, inclusive interface. This tool has
        the potential to meaningfully enhance document comprehension, especially
        for materials that include complex layouts or images, which traditional
        OCR systems often struggle to interpret. However, the benefits hinge on
        careful and ethical implementation, especially regarding fairness,
        accuracy, and user privacy.
      </p>
      <p>
        While non-ML alternatives (e.g., basic OCR paired with text-to-speech)
        exist, they typically fall short in terms of interpreting image-heavy or
        structurally complex documents. Our approach, using advanced
        transformer-based models, is better suited for delivering contextual
        understanding of both visual and textual elements.
      </p>
      <p>
        Our team brings together individuals from varied technical and creative
        backgrounds, including computer science and design. However, none of us
        personally experience vision impairments. To mitigate this limitation,
        we plan to involve low-vision users throughout the development and
        testing process, ensuring that their feedback informs key design
        decisions. To handle mistakes or edge cases, we will incorporate an
        in-app feedback mechanism, allowing users to flag errors in text
        extraction or image descriptions. These reports will be reviewed
        regularly, and critical issues will be prioritized in system updates.
        Transparency will be maintained through accessible documentation
        outlining how the system functions, the models involved, and their known
        limitations.
      </p>
      <h3>Data Curation and Use</h3>
      <p>
        Our tool relies primarily on Tesseract OCR models and the SynthText
        dataset, which offer a solid starting point for general-purpose text
        recognition. These sources are well-established in the field and provide
        a strong baseline for printed text in structured layouts. However, they
        have limitations when it comes to less conventional document types—such
        as handwritten notes, tactile documents like Braille, or heavily
        cluttered layouts—which could reduce the effectiveness of our system in
        those contexts.
      </p>
      <p>
        In addition to OCR, we use image captioning models like BLIP and Donut,
        which are trained on large-scale internet datasets. While these models
        bring powerful capabilities for describing images, their training data
        may introduce several kinds of bias. One key issue is language bias,
        given the predominance of English and Latin-based scripts in both the
        OCR and captioning datasets. Another concern is layout bias, since
        synthetic datasets like SynthText may not fully capture the variability
        and disorganization of real-world documents, such as medical forms or
        personal notes. Lastly, the captioning models may reflect stereotypical
        assumptions embedded in their training data, including inaccurate
        gendering or cultural misrepresentation in image descriptions.
      </p>
      <p>
        These challenges highlight the importance of validating our tool with
        real-world user data and maintaining awareness of how dataset
        limitations might affect performance across different user groups and
        document types.
      </p>
      <h3>Impact Assessment</h3>
      <p>
        Our main ethical focus is on how the tool might affect users if it were
        actually deployed in the real world. One consideration is user reliance:
        even though we’re designing this to support low-vision users, we
        wouldn’t want the system to unintentionally replace their own ways of
        navigating content. To address this, we discussed interactive features
        like a “hints” or “progressive reveal” mode, but due to time and scope,
        we’re focusing on just making the basic tool work well first.
      </p>
      <p>
        Another potential issue is how the tool would handle documents with high
        stakes—like medical instructions, financial documents, or anything that
        requires high accuracy. We’re not aiming for full reliability in those
        contexts at this stage, but we imagine future versions could include
        accuracy warnings or prompts to verify results with a sighted assistant.
      </p>
      <p>
        Finally, we know our current system may not perform equally well on all
        document types—especially handwritten notes, complex diagrams, or
        non-English content. We’d need more testing and feedback from real users
        to understand how well the system generalizes. Long-term, it would be
        important to track those gaps and make sure improvements don’t benefit
        just one kind of user or document.
      </p>

      <h2>Related Work</h2>
      <p>
        Several previous efforts have informed and shaped our project, yet each
        contains gaps that our solution specifically targets and resolves. For
        example, Smith's ImageAssist (2021) <a href="#ref5">[5]</a> innovatively
        enabled visually impaired users to interactively explore images but
        focused primarily on isolated, exploratory interactions rather than
        integrated interpretation of mixed textual-visual content within
        documents. Our application explicitly advances beyond this by providing
        cohesive, contextually integrated descriptions for entire documents,
        thus filling a crucial accessibility gap left by ImageAssist.
      </p>
      <p>
        Similarly, Vijayanarayanan et al. (2023)
        <a href="#ref6">[6]</a> successfully combined OCR with text-to-speech
        for textual accessibility, effectively demonstrating OCR's strengths.
        However, their solution overlooks the critical task of interpreting
        visual elements and complex document layouts. By specifically
        integrating sophisticated HuggingFace transformer-based image captioning
        models, our approach directly resolves this limitation, offering
        comprehensive visual-textual integration essential for complete document
        understanding.
      </p>
      <p>
        Similarly, Vijayanarayanan et al. (2023)<a href="#ref6">[6]</a>
        demonstrated the effectiveness of combining Optical Character
        Recognition (OCR) and text-to-speech to support visually impaired users
        in efficiently processing textual content. While their approach
        highlights the strengths of OCR technology, it lacks the capability to
        interpret images or complex document layouts. Our project addresses this
        critical gap by integrating advanced HuggingFace transformer models
        specifically optimized for context-rich image captioning, ensuring
        visual elements are fully integrated into the document’s accessible
        description.
      </p>
      <p>
        Moreover, Bodi et al. (2021) <a href="#ref7">[7]</a> demonstrated
        powerful AI-driven contextual descriptions within dynamic visual
        environments (videos), developing tools such as NarrationBot and
        InfoBot. Despite their significance, these tools remain specialized for
        dynamic contexts, leaving the accessibility needs of static, image-rich
        documents largely unaddressed. Our solution specifically adapts
        AI-driven narrative generation techniques to static documents, directly
        extending AI capabilities into this underserved yet essential domain.
      </p>
      <p>
        Collectively, our project synthesizes these foundational insights while
        explicitly resolving their individual limitations, delivering a uniquely
        comprehensive, robust accessibility solution tailored precisely to the
        critical needs of low-vision users interpreting complex visual-textual
        documents.
      </p>

      <h2>Methods</h2>
      <p>
        This section outlines the methodology we followed to design and
        implement our AI-powered document reader for low-vision users. Our goal
        was to build a system that could interpret both textual and visual
        content in documents through the integration of optical character
        recognition (OCR) and transformer-based vision-language models (VLMs).
        To ensure a user-friendly and accessible interface, we also incorporated
        real-time interaction features using popular Python web frameworks.
        Below, we detail the specific tools, models, datasets, and technical
        strategies that powered our system.
      </p>
      <p>
        To extract text from images and PDFs, we selected Tesseract OCR due to
        its open-source accessibility and proven performance in recognizing
        characters from various fonts and languages. Tesseract is particularly
        effective in converting scanned or image-based text into digital
        content, which is critical for users who depend on screen readers or
        text-to-speech technology. The OCR pipeline in Tesseract includes
        several preprocessing stages such as layout analysis, line segmentation,
        and blob detection. After segmenting the text into lines and characters,
        the system proceeds through a two-pass recognition process. In the first
        pass, it generates hypotheses for character sequences based on grouping
        and shape similarity. These preliminary results are then used to train a
        temporary classifier that reprocesses the text in the second pass,
        leading to improved accuracy, especially for noisy or ambiguous input.
      </p>
      <p>
        A major advantage of Tesseract lies in its integration of deep learning
        techniques. As of version 4.0, Tesseract incorporates Long Short-Term
        Memory (LSTM) networks, which are a type of Recurrent Neural Network
        (RNN) capable of modeling sequential dependencies. LSTMs are
        particularly useful in OCR tasks as they can maintain memory across
        longer spans of text, making them ideal for handling paragraph-level
        content with inconsistent formatting or irregular character placement.
        The internal architecture of an LSTM cell is designed around three
        primary gates—forget, input, and output—that control the flow of
        information through the cell. These gates use nonlinearities such as the
        sigmoid and hyperbolic tangent functions to regulate memory updates and
        output generation.
      </p>
      <img
        class="imgg"
        src="WebP Image.webp"
        alt="LSTM Cell Architecture Diagram showing input, forget, and output gates with memory flow."
      />
      <b>
        Figure 1: Architecture of an LSTM cell showing how input vector, memory,
        and hidden states flow through various gates (forget, input, and output)
        using sigmoid and tanh functions.
      </b>
      <p>
        This design enables the model to focus on relevant parts of the input
        while filtering out noise—a crucial capability when processing complex
        or cluttered documents. The use of LSTMs thus improves the contextual
        accuracy of text recognition, especially when OCR must be applied to
        content with variable font sizes, styles, or alignments.
      </p>
      <p>
        To expand beyond simple text extraction, we implemented visual
        interpretation capabilities using transformer-based models from Hugging
        Face. We used two pretrained models: BLIP (Bootstrapped Language-Image
        Pretraining) and Donut (Document Understanding Transformer). BLIP excels
        at generating descriptive captions for natural images, while Donut is
        optimized for understanding structured document layouts, such as
        receipts, invoices, or forms. These models rely on attention mechanisms
        within a multimodal transformer architecture, allowing them to process
        and align visual features with linguistic representations. This enables
        them to generate semantically rich and context-aware descriptions, which
        are especially valuable for low-vision users who cannot rely on visual
        cues.
      </p>
      <p>
        The integration of these models was managed through an interactive web
        interface, which we initially developed using Gradio. Gradio allowed for
        quick prototyping and seamless integration with the Hugging Face model
        APIs, making it an ideal choice for early-stage experimentation.
        However, for the final deployment, we transitioned to Streamlit to take
        advantage of its more robust layout management and hierarchical content
        rendering. Streamlit also enabled us to prioritize accessibility by
        supporting semantic HTML elements, keyboard navigation, and customizable
        visual themes. The resulting web application allows users to upload
        documents or images, processes them through both the OCR and captioning
        pipelines, and outputs structured textual descriptions that can be read
        aloud using a text-to-speech module.
      </p>
      <p>
        To evaluate our models and measure the system’s effectiveness, we used
        data from the SynthText dataset as well as Tesseract’s own training
        corpus. The SynthText dataset contains over 800,000 synthetic text
        images that simulate real-world distortions, including varied
        backgrounds, fonts, and orientations. This diversity allowed us to test
        the robustness of the OCR pipeline across a wide range of visual
        conditions. We evaluated OCR performance using precision, recall, and F1
        scores on extracted words. For the vision-language models, we assessed
        caption quality using BLEU and ROUGE scores, comparing the generated
        descriptions to human-annotated ground truths. We also conducted manual
        reviews to ensure contextual relevance and clarity in the captions.
      </p>
      <p>
        Postprocessing was a crucial step in refining the user experience. OCR
        outputs were cleaned to remove unwanted formatting artifacts and
        normalize punctuation and spacing. Image captions were filtered to
        reduce repetition and vague phrasing. In the audio output pipeline, we
        optimized pronunciation and pacing to improve intelligibility for screen
        reader users. These steps helped align the system’s output with user
        expectations for both clarity and accessibility.
      </p>
      <p>
        Finally, our development process was iterative and guided by a
        commitment to accessibility. Although we have not yet conducted formal
        user testing, we plan to incorporate feedback mechanisms within the
        application. Users will be able to flag incorrect outputs, and these
        reports will help us prioritize improvements in future versions. Our
        goal is to ensure that the system continues to evolve based on
        real-world needs, particularly those of the low-vision community.
      </p>

      <h2>Results</h2>
      <video
        src="Screen Recording 2025-04-19 at 8.58.04 PM.mov"
        controls
        width="640"
      >
        Your browser does not support the video tag.
      </video>
      <p>
        To evaluate our system's ability to extract and vocalize textual content
        from documents, we built and tested a fully functional web-based
        interface using Gradio. The resulting tool, titled Image to Speech
        Converter, allows users to upload images or PDFs, extract embedded text
        using Tesseract OCR, and receive audio playback using Google
        Text-to-Speech (gTTS). Users can also select from different English
        voice types, including Female US, Female AU, and UK English. The system
        was tested using a range of document types, including clean text, text
        with visual noise, and longer passages.
      </p>
      <img src="images copy.jpeg" alt="" />
      <b
        >An image with a noisy background highlighted one of the limitations of
        standard OCR, as Tesseract struggled to return clean results due to the
        speckled background noise.
      </b>
      <img src="sample copy.jpg" alt="" />
      <b
        >A simple, high-contrast image was accurately read, showing strong
        performance on ideal inputs.
      </b>

      <p>
        Each of these inputs generated an MP3 file stored in the outputs/
        folder, confirming successful audio synthesis. These audio files are
        named based on the uploaded filename and the selected voice, e.g.,
        sample_female_us.mp3. A screenshot of the output directory (shown below)
        verifies the system's ability to generate multiple outputs across
        testing sessions.
      </p>
      <p>
        The frontend was built with Gradio's Blocks API, which provided
        flexibility in creating a modular interface. Features include a file
        upload component, voice type dropdown, audio preview, text display of
        processing status, and a download button for offline access to generated
        speech. The UI was further customized with CSS to improve readability
        and accessibility for users with low vision.
      </p>
      <p>
        From the backend, the system successfully handled both .jpg/.png image
        inputs and .pdf documents. PDF processing was handled by pdf2image,
        converting pages into images before passing them to Tesseract. Text was
        then sent to gTTS for synthesis using the selected language and accent.
        Voice customization was achieved using the tld parameter in gTTS,
        allowing regional variations like US (.com), UK (.co.uk), and Australia
        (.com.au).
      </p>
      <p>
        The extracted audio was clear and accurate for all clean inputs. As
        expected, performance degraded slightly with visually noisy inputs,
        reinforcing one of the known limitations of OCR models like Tesseract
        when faced with cluttered backgrounds or poor contrast. This finding
        supports the need for preprocessing techniques or fallback strategies in
        future versions of the tool.
      </p>
      <h2>Reflection</h2>
      <p>
        Working on the AI-Powered Document Reader pushed our team to think
        deeply not only about what machine learning tools can do, but how they
        can be shaped into something that truly serves users—particularly those
        with low vision. Across both the backend and frontend, we encountered
        real challenges that taught us the importance of technical adaptability,
        responsible development practices, and intentional design for
        accessibility.
      </p>
      <p>
        From a backend perspective, one key takeaway was the importance of
        choosing the right tools for the scope of the project. As one teammate
        reflected, we learned to research and read documentation carefully
        before adopting any new technology. In the early stages, we sometimes
        prioritized novelty over practicality—trying frameworks or models
        without fully understanding their dependencies or long-term
        maintainability. This approach quickly led to compatibility issues and
        unnecessary complexity. Eventually, we focused on lightweight,
        well-supported technologies with strong community backing. Tools like
        gTTS, pytesseract, and Gradio offered just enough functionality without
        overloading the stack, and their active support ecosystems helped us
        avoid getting stuck with unpatched code or security risks.
      </p>
      <p>
        On the frontend, we designed our UI with low-vision accessibility as a
        central concern. We used bold text, dark backgrounds, and high-contrast
        elements to improve readability, and we offered multiple voice
        options—including regional accents—to give users greater control over
        how they engage with the content. As one teammate noted, even small
        design decisions—like enabling font customizations or providing
        adjustable speech speed—can make a huge difference in user experience.
        These choices weren’t just cosmetic; they reflected a deeper goal: to
        empower users to navigate information in the way that feels most
        intuitive and comfortable to them.
      </p>
      <p>
        Drawing from accessibility research and insights such as those found in
        Morris et al., 2016, we also identified areas for improvement that we
        plan to explore in future iterations. For example, enabling smarter
        magnification, intelligent color inversion, or hybrid screen reader
        modes could reduce fatigue for users who toggle between visual and audio
        inputs. Implementing gesture simplification and building a clear
        onboarding experience would also lower the barrier for first-time users.
        One powerful idea that stood out was the need to respect social
        discretion—that is, designing accessibility features that blend
        seamlessly into the interface so users don’t feel singled out or
        visually marked by their adaptations.
      </p>
      <p>
        As a group, we also came to understand that accessibility isn't just a
        checklist—it’s a mindset. Inclusive design requires constant
        questioning: Who is being served by this interface? What assumptions are
        we making about how people access information? And how can we give users
        more autonomy over their experience? These questions challenged us to
        move beyond functional success into ethical and human-centered problem
        solving.
      </p>
      <p>
        In the future, we hope to incorporate real user testing with blind and
        low-vision individuals to validate our design assumptions. We’d also
        like to explore features like dynamic font scaling, touchless
        navigation, and environment-aware theming (e.g., auto-switching based on
        lighting conditions). Ultimately, this project showed us that AI is most
        powerful when it extends access, not just when it automates tasks—and
        that true innovation happens when technical creativity meets deep
        listening to user needs.
      </p>

      <h2>References</h2>
      <p id="ref1">
        [1] Tesseract OCR.
        <a href="https://github.com/tesseract-ocr/tesseract"
          >https://github.com/tesseract-ocr/tesseract</a
        >
        <a href="#top">(Back to text)</a>
      </p>
      <p id="ref2">
        [2] HuggingFace Transformers.
        <a href="https://huggingface.co/transformers/"
          >https://huggingface.co/transformers/</a
        >
        <a href="#top">(Back to text)</a>
      </p>
      <p id="ref3">
        [3] Gradio.
        <a href="https://gradio.app/">https://gradio.app/</a>
        <a href="#top">(Back to text)</a>
      </p>
      <p id="ref4">
        [4] Streamlit.
        <a href="https://streamlit.io/">https://streamlit.io/</a>
        <a href="#top">(Back to text)</a>
      </p>
      <p id="ref5">
        [5] Smith, B. A. (2021). "ImageAssist: Enhancing Image Understanding for
        Low Vision Users." Proceedings of CHI.
        <a href="#top">(Back to text)</a>
      </p>
      <p id="ref6">
        [6] Vijayanarayanan et al. (2023). "Image Processing Based on Optical
        Character Recognition with Text-to-Speech for Visually Impaired."
        <em>Journal of Scientific and Engineering Research, 6(4).</em>
        <a href="#top">(Back to text)</a>
      </p>
      <p id="ref7">
        [7] Bodi et al. (2021). "Automated Video Description for Blind and Low
        Vision Users."
        <em
          >Proceedings of CHI Conference on Human Factors in Computing
          Systems.</em
        >
        <a href="#top">(Back to text)</a>
      </p>
    </div>
  </body>
</html>
